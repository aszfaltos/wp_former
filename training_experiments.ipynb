{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c29d49-35c0-426e-a5ba-3c1273947d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:00:47.222426871Z",
     "start_time": "2023-12-08T14:00:44.459402186Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data_handling.data_loader import load_mavir_data\n",
    "from trainer_lib import Grid, grid_search\n",
    "from models import Transformer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dee256-92d0-44f9-bdc1-c5a5416d347f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:00:47.277362057Z",
     "start_time": "2023-12-08T14:00:47.225015859Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_mavir_data('data/mavir_data/mavir.csv')\n",
    "df['Power'] = utils.min_max_norm(df['Power'])\n",
    "sample = utils.sample(df, 10000, start_idx=0)\n",
    "\n",
    "# imfs, residue = utils.apply_eemd(sample['Power'].to_numpy(), spline_kind='akima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002f67f4-272a-4c38-8547-2698b67defce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:00:47.295860416Z",
     "start_time": "2023-12-08T14:00:47.283457681Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = np.array(sample['Power'].to_numpy()[...,np.newaxis], dtype=np.float32)\n",
    "# training_data_decomp = np.array(np.c_[imfs.transpose(), residue[...,np.newaxis]], dtype=np.float32)\n",
    "# training_data_decomp_rem = np.array(np.c_[imfs[1:].transpose(), residue[...,np.newaxis]], dtype=np.float32)\n",
    "# training_data_combined = np.array(np.c_[sample['Power'].to_numpy()[...,np.newaxis], imfs.transpose(), residue[...,np.newaxis]], dtype=np.float32)\n",
    "# training_data_combined_rem = np.array(np.c_[sample['Power'].to_numpy()[...,np.newaxis], imfs[1:].transpose(), residue[...,np.newaxis]], dtype=np.float32)\n",
    "# print(training_data.shape)\n",
    "# print(training_data.dtype)\n",
    "# print(training_data_decomp.shape)\n",
    "# print(training_data_decomp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc320372-d6ea-492f-8a07-7638c6e948cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:04:00.106859212Z",
     "start_time": "2023-12-08T14:00:47.316183023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8806, Validation size: 978\n",
      "Epoch: 1; Learning rate: [0.0001045]; Train - MSE: 0.4994566842412853; Eval - MSE: 0.20668545480423822, RMSE: 0.45462672029285545, MAE: 0.41446512937545776\n",
      "Epoch: 2; Learning rate: [0.000204]; Train - MSE: 0.12391752575090215"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m grid \u001B[38;5;241m=\u001B[39m Grid(params)\n\u001B[1;32m     16\u001B[0m names \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mgenerate_name(\u001B[38;5;28mlen\u001B[39m(grid), \u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m models \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./trained/regular/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/trainer_lib/grid_search.py:30\u001B[0m, in \u001B[0;36mgrid_search\u001B[0;34m(grid, names, model_type, data, batch_size, epochs, split, step_size, root_path)\u001B[0m\n\u001B[1;32m     27\u001B[0m     name \u001B[38;5;241m=\u001B[39m names[idx]\n\u001B[1;32m     28\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m Trainer(model, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(path, name), batch_size, epochs)\n\u001B[0;32m---> 30\u001B[0m     \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     models\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: name,\n\u001B[1;32m     32\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: trainer\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m     33\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: params,\n\u001B[1;32m     34\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m'\u001B[39m: trainer\u001B[38;5;241m.\u001B[39mmetrics})\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m models\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/trainer_lib/trainer.py:55\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_data, valid_data)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; Learning rate: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mscheduler\u001B[38;5;241m.\u001B[39mget_last_lr()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; Train - MSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     52\u001B[0m     end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMSE\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m---> 55\u001B[0m stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m checkpoint(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_path, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stop:\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/trainer_lib/trainer.py:71\u001B[0m, in \u001B[0;36mTrainer._evaluate\u001B[0;34m(self, data_loader, criterion, early_stopper)\u001B[0m\n\u001B[1;32m     69\u001B[0m out \u001B[38;5;241m=\u001B[39m ones\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(tgt_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[0;32m---> 71\u001B[0m     out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mconcat((ones, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     72\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(out[:, \u001B[38;5;241m1\u001B[39m:], tgt_data)\n\u001B[1;32m     73\u001B[0m eval_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_loader)\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/models/transformer.py:176\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, src, tgt)\u001B[0m\n\u001B[1;32m    174\u001B[0m enc_output \u001B[38;5;241m=\u001B[39m src_pos_encoded\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m enc_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menc_layers:\n\u001B[0;32m--> 176\u001B[0m     enc_output \u001B[38;5;241m=\u001B[39m \u001B[43menc_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m dec_output \u001B[38;5;241m=\u001B[39m tgt_pos_encoded\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dec_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdec_layers:\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/models/transformer.py:93\u001B[0m, in \u001B[0;36mEncoderLayer.forward\u001B[0;34m(self, x, mask)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 93\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn_heads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attn_output))\n\u001B[1;32m     95\u001B[0m     ff_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mff(x)\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/models/transformer.py:49\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, q, k, v, mask)\u001B[0m\n\u001B[1;32m     46\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_heads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_k(k))\n\u001B[1;32m     47\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_heads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_v(v))\n\u001B[0;32m---> 49\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_o(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcombine_heads(attn_output))\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Documents/projects/wp_former/models/transformer.py:23\u001B[0m, in \u001B[0;36mMultiHeadAttention.scaled_dot_product_attention\u001B[0;34m(self, q, r, v, mask)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscaled_dot_product_attention\u001B[39m(\u001B[38;5;28mself\u001B[39m, q, r, v, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m# outer product of Q and K normalized\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     attn_scores \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m---> 23\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k)\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     26\u001B[0m         attn_scores \u001B[38;5;241m=\u001B[39m attn_scores\u001B[38;5;241m.\u001B[39mmasked_fill(mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1e9\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'src_size' : [1*8],\n",
    "    'tgt_size' : [1*1],\n",
    "    'd_model' : [256], #, 256, 512],\n",
    "    'num_heads' : [2], # , 4, 8],\n",
    "    'num_layers' : [2], # , 2, 3],\n",
    "    'd_ff' : [512], # , 1024, 2048],\n",
    "    'enc_seq_length' : [24], # , 96],\n",
    "    'dec_seq_length' : [1],\n",
    "    'enc_window': [8],\n",
    "    'dec_window': [1],\n",
    "    'dropout' : [0.2], # , 0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "grid = Grid(params)\n",
    "names = utils.generate_name(len(grid), 42)\n",
    "models = grid_search(grid, names, Transformer, training_data, epochs=50, split=0.1, root_path='./trained/regular/')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0af9384-1b44-487b-ac11-c616b6f0c865",
   "metadata": {},
   "source": [
    "params = {\n",
    "'src_size' : [11],\n",
    "'tgt_size' : [11],\n",
    "'d_model' : [128], #, 256, 512],\n",
    "'num_heads' : [2], # , 4, 8],\n",
    "'num_layers' : [1], # , 2, 3],\n",
    "'d_ff' : [512], # , 1024, 2048],\n",
    "'enc_seq_length' : [72], # , 96],\n",
    "'dec_seq_length' : [24],\n",
    "'dropout' : [0.2], # , 0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "grid = Grid(params)\n",
    "\n",
    "models_decomp = grid_search(grid, Transformer, training_data_decomp, training_data_decomp, epochs=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "633cafa0-bff5-4132-8852-bba4f1772378",
   "metadata": {},
   "source": [
    "params = {\n",
    "'src_size' : [12],\n",
    "'tgt_size' : [1],\n",
    "'d_model' : [128], #, 256, 512],\n",
    "'num_heads' : [2], # , 4, 8],\n",
    "'num_layers' : [1], # , 2, 3],\n",
    "'d_ff' : [512], # , 1024, 2048],\n",
    "'enc_seq_length' : [72], # , 96],\n",
    "'dec_seq_length' : [24],\n",
    "'dropout' : [0.2], # , 0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "grid = Grid(params)\n",
    "\n",
    "models_decomp_combined = grid_search(grid, Transformer, training_data_combined, training_data, epochs=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b014c52a-2490-43bb-a418-0e802c903943",
   "metadata": {},
   "source": [
    "params = {\n",
    "'src_size' : [11],\n",
    "'tgt_size' : [1],\n",
    "'d_model' : [128], #, 256, 512],\n",
    "'num_heads' : [2], # , 4, 8],\n",
    "'num_layers' : [1], # , 2, 3],\n",
    "'d_ff' : [512], # , 1024, 2048],\n",
    "'enc_seq_length' : [72], # , 96],\n",
    "'dec_seq_length' : [24],\n",
    "'dropout' : [0.2], # , 0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "grid = Grid(params)\n",
    "\n",
    "models_decomp_combined_rem = grid_search(grid, Transformer, training_data_combined_rem, training_data, epochs=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c1de78a-c276-45f7-9b50-c12c29510d35",
   "metadata": {},
   "source": [
    "params = {\n",
    "'src_size' : [10],\n",
    "'tgt_size' : [10],\n",
    "'d_model' : [128], #, 256, 512],\n",
    "'num_heads' : [2], # , 4, 8],\n",
    "'num_layers' : [1], # , 2, 3],\n",
    "'d_ff' : [512], # , 1024, 2048],\n",
    "'enc_seq_length' : [72], # , 96],\n",
    "'dec_seq_length' : [24],\n",
    "'dropout' : [0.2], # , 0.1, 0.15, 0.2],\n",
    "}\n",
    "\n",
    "grid = Grid(params)\n",
    "\n",
    "models_decomp_rem = grid_search(grid, Transformer, training_data_decomp_rem, training_data_decomp_rem, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9fc5f-cf20-477a-a64c-a7ebc35536b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:04:00.122911966Z",
     "start_time": "2023-12-08T14:04:00.113038683Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#for model in models_decomp_combined:\n",
    "#    plt.plot(model['metrics']['eval_loss'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - combined')\n",
    "\n",
    "#for model in models_decomp_combined_rem:\n",
    "#    plt.plot(model['metrics']['eval_loss'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - combined rem')\n",
    "\n",
    "#for model in models_decomp_rem:\n",
    "#    plt.plot(model['metrics']['eval_loss'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - rem')\n",
    "\n",
    "#for model in models_decomp:\n",
    "#    plt.plot(model['metrics']['eval_loss'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - decomp')\n",
    "\n",
    "for model in models:\n",
    "    plt.plot(model['metrics']['eval']['MSE'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - normal - eval')\n",
    "    plt.plot(model['metrics']['train']['MSE'], label=f'{model[\"name\"]} - {model[\"params\"][\"enc_seq_length\"]} - normal - train')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bc098-3a4a-493b-90a1-ba731694b264",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-08T14:04:00.122748851Z"
    }
   },
   "outputs": [],
   "source": [
    "from trainer_lib.datasets import TimeSeriesWindowedTensorDataset, TimeSeriesWindowedDatasetConfig\n",
    "\n",
    "shift, look_back, pred = 50, 24, 1\n",
    "\n",
    "dataset = TimeSeriesWindowedTensorDataset(training_data, TimeSeriesWindowedDatasetConfig((8, 1), (24, 1), 1))\n",
    "ones = torch.ones(1, 1, dataset[0][1].shape[-1])\n",
    "for model in models:\n",
    "    model['model'].eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        ground_truth = []\n",
    "        predicted = []\n",
    "        for shift_offset in range(shift, shift+24, 1):\n",
    "            out = ones\n",
    "            for i in range(pred):\n",
    "                out = torch.concatenate((ones, model['model'](dataset[shift_offset][0].unsqueeze(0), out)), dim=1)\n",
    "        \n",
    "            predicted.append(dataset.get_sequence_from_y_windows(out[:, 1:, :].detach()))\n",
    "            ground_truth.append(dataset.get_sequence_from_y_windows(dataset[shift_offset][1]))\n",
    "    \n",
    "    predicted = np.array(predicted).reshape(24)\n",
    "    ground_truth = np.array(ground_truth).reshape(24)\n",
    "    plt.plot(ground_truth, label='ground truth')\n",
    "    plt.plot(predicted, label='24h rolling one step')\n",
    "    \n",
    "    #output = model['model'](\n",
    "    #    dataset[shift][0].unsqueeze(0),  \n",
    "    #    torch.concat((ones, dataset[shift][1][:-1, :].unsqueeze(0)), dim=1)\n",
    "    #)\n",
    "    #\n",
    "    #plt.plot(torch.concat(\n",
    "    #    (dataset.get_sequence_from_x_windows(dataset[shift][0]), \n",
    "    #     dataset.get_sequence_from_y_windows(dataset[shift][1])), dim=0), label='original')\n",
    "    #plt.plot(\n",
    "    #    torch.concatenate(\n",
    "    #        (dataset.get_sequence_from_x_windows(dataset[shift][0]),\n",
    "    #         dataset.get_sequence_from_y_windows(output[:, :, :].detach())), dim=0), \n",
    "    #    label='full access - normal'\n",
    "    #)\n",
    "    #plt.plot(\n",
    "    #    torch.concatenate(\n",
    "    #        (dataset.get_sequence_from_x_windows(dataset[shift][0]), \n",
    "    #         dataset.get_sequence_from_y_windows(out[:, 1:, :].detach())), dim=0),\n",
    "    #    label='predicted - normal'\n",
    "    #)\n",
    "    \n",
    "#for model in models_decomp:\n",
    "#    out = torch.ones(1,1,11)\n",
    "#    for _ in range(25):\n",
    "#        output = model['model'](torch.tensor(training_data_decomp[np.newaxis, 0:72,:]), out)# torch.tensor(training_data[np.newaxis, 73:97,:]))# torch.zeros((1, 24, 1)))\n",
    "#        out = torch.concatenate((out, output[:,-1,:].unsqueeze(1)), axis=1)\n",
    "#    plt.plot(out[:, 1:-1, :].detach().reshape((24,11)).sum(-1), label='predicted - decomp')\n",
    "\n",
    "#for model in models_decomp_rem:\n",
    "#    out = torch.ones(1,1,10)\n",
    "#    for _ in range(25):\n",
    "#        output = model['model'](torch.tensor(training_data_decomp_rem[np.newaxis, 0:72,:]), out)# torch.tensor(training_data[np.newaxis, 73:97,:]))# torch.zeros((1, 24, 1)))\n",
    "#        out = torch.concatenate((out, output[:,-1,:].unsqueeze(1)), axis=1)\n",
    "#    plt.plot(out[:, 1:-1, :].detach().reshape((24,10)).sum(-1), label='predicted - rem')\n",
    "\n",
    "#for model in models_decomp_combined:\n",
    "#    out = torch.ones(1,1,1)\n",
    "#    for _ in range(25):\n",
    "#        output = model['model'](torch.tensor(training_data_combined[np.newaxis, 0:72,:]), out)# torch.tensor(training_data[np.newaxis, 73:97,:]))# torch.zeros((1, 24, 1)))\n",
    "#        out = torch.concatenate((out, output[:,-1,:].unsqueeze(1)), axis=1)\n",
    "#    plt.plot(out[:, 1:-1, :].detach().reshape((24,)), label='predicted - combined')\n",
    "\n",
    "#for model in models_decomp_combined_rem:\n",
    "#    out = torch.ones(1,1,1)\n",
    "#    for _ in range(25):\n",
    "#        output = model['model'](torch.tensor(training_data_combined_rem[np.newaxis, 0:72,:]), out)# torch.tensor(training_data[np.newaxis, 73:97,:]))# torch.zeros((1, 24, 1)))\n",
    "#        out = torch.concatenate((out, output[:,-1,:].unsqueeze(1)), axis=1)\n",
    "#    plt.plot(out[:, 1:-1, :].detach().reshape((24,)), label='predicted - combined rem')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a5eb4-a47c-4078-b5d2-fc73ab2d42a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:04:00.191794978Z",
     "start_time": "2023-12-08T14:04:00.128449671Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
